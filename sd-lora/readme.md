# ğŸ’ é‡‘ä¸çŒ´ LoRA + å§¿åŠ¿æ§åˆ¶ å…¨æµç¨‹

> ä»ã€Œæ‹ç…§ç‰‡ â†’ LoRA è®­ç»ƒ â†’ å§¿åŠ¿æ§åˆ¶ç”Ÿæˆã€ä¸€æ­¥ä¸å°‘ï¼Œå…¨éƒ¨å¯è·‘ã€‚

---

## ğŸ¯ é¡¹ç›®ç›®æ ‡
1. ç”¨ **30 å¼ é‡‘ä¸çŒ´ç…§ç‰‡** è®­ç»ƒ **SDXL LoRA**
2. ç”¨ **OpenPose æ§åˆ¶å§¿åŠ¿**ï¼Œç”Ÿæˆã€Œ**å§¿åŠ¿é”æ­» + æ¦‚å¿µä¿ç•™**ã€çš„é‡‘ä¸çŒ´å›¾
3. **CLIP score å½“åœ¨çº¿ Gate**ï¼Œ>0.30 è‡ªåŠ¨æ”¾è¡Œ

---

## ğŸ“¦ ç¡¬ä»¶ & ç¯å¢ƒ
- **GPU**ï¼šâ‰¥12 GBï¼ˆSDXL fp16ï¼‰
- **ç³»ç»Ÿ**ï¼šLinux / WSL2 / macOS
- **ç¯å¢ƒ**ï¼š
  ```bash
  pip install torch torchvision transformers diffusers accelerate controlnet-aux xformers
  ```

---

## ğŸ“ é¡¹ç›®ç»“æ„
```
golden-monkey-lora/
â”œâ”€â”€ data/                     # åŸå§‹ç…§ç‰‡ + txt
â”‚   â”œâ”€â”€ golden_monkey_001.png
â”‚   â””â”€â”€ golden_monkey_001.txt
â”œâ”€â”€ models/                   # ä¸‹è½½çš„æƒé‡
â”‚   â”œâ”€â”€ stable-diffusion/     # SDXL åŸºç¡€
â”‚   â”œâ”€â”€ sd-controlnet-openpose-sdxl/
â”‚   â””â”€â”€ annotator/
â”œâ”€â”€ out-lora/                 # è®­ç»ƒè¾“å‡º
â”œâ”€â”€ generate_pose.py          # å§¿åŠ¿æ§åˆ¶ç”Ÿæˆ
â”œâ”€â”€ clip_score.py             # CLIP score + æ®‹å·®å›¾
â”œâ”€â”€ generate_sd_lora.py       # å¾®è°ƒè„šæœ¬
â””â”€â”€ README.md                 # æœ¬æ–‡ä»¶
```

---

## ğŸ’ 1. æ•°æ®å‡†å¤‡ï¼ˆ30 å¼ å›¾å³å¯ï¼‰
### â‘  æ‹ç…§/æ‰¾å›¾
- **ç»Ÿä¸€ 1024Ã—1024**ï¼Œæ­£é¢/ä¾§é¢/è·³è·ƒå§¿æ€
- å‘½åï¼š`golden_monkey_001.png â€¦ 030.png`

### â‘¡ æ‰“æ ‡ç­¾
- æ¯å›¾åŒå†…å®¹ txtï¼š**`golden monkey`**
- æ”¾åŒä¸€ç›®å½•ï¼š`data/`

---

## ğŸ§ª 2. è®­ç»ƒ LoRAï¼ˆSDXLï¼‰
```bash
accelerate launch train_sdxl_lora.py \
  --pretrained_model_name_or_path ./models/stable-diffusion \
  --train_data_dir ./data \
  --output_dir ./out-lora \
  --resolution 1024 --train_batch_size 1 --gradient_checkpointing \
  --max_train_steps 500 --checkpointing_steps 100 \
  --rank 32 --lora_alpha 16 --train_text_encoder \
  --use_spacetime --seed 42
```
**äº§å‡º**ï¼š`out-lora/checkpoint-500/lora.safetensors`

---

## ğŸ•º 3. å§¿åŠ¿æ§åˆ¶ç”Ÿæˆï¼ˆOpenPose é”å§¿åŠ¿ï¼‰
### â‘  ä¸‹è½½ ControlNetï¼ˆSDXL ç‰ˆï¼‰ä»¥åŠå§¿åŠ¿æ ‡æ³¨
```bash
modelscope download --model lllyasviel/Annotators
mkdir -p models/controlnet/sd-controlnet-openpose-sdxl
wget https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0/resolve/main/diffusion_pytorch_model.safetensors -O models/controlnet/sd-controlnet-openpose-sdxl/diffusion_pytorch_model.safetensors
wget https://huggingface.co/thibaud/controlnet-openpose-sdxl-1.0/raw/main/config.json -O models/controlnet/sd-controlnet-openpose-sdxl/config.json
```

### â‘¡ ç”Ÿæˆè„šæœ¬
```python
# generate_pose.py
from PIL import Image
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from controlnet_aux import OpenposeDetector
import torch

# 1. åŠ è½½ ControlNet
controlnet = ControlNetModel.from_pretrained(
    "./models/controlnet-openpose-sdxl",
    torch_dtype=torch.float16
)

# 2. åŠ è½½ SDXL + ControlNet ç®¡é“
pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
    base_model,
    controlnet=controlnet,
    torch_dtype=torch.float16,
    variant="fp16"
).to(device)

# 3. åŠ è½½ LoRAï¼ˆWebUI æ ¼å¼ï¼‰
#pipe.load_lora_weights(lora_path)

# 4. åŠ è½½å§¿åŠ¿å›¾
pose_image = Image.open(pose_path).convert("RGB").resize((width, height))

# 5. ç”Ÿæˆ
generator = torch.Generator(device).manual_seed(seed)
image = pipe(
    prompt=prompt,
    negative_prompt=negative,
    image=pose_image,
    width=width,
    height=height,
    num_inference_steps=num_steps,
    guidance_scale=guidance,
    generator=generator,
    controlnet_conditioning_scale=1.0,  # å§¿åŠ¿å¼ºåº¦ 0-1
).images[0]

# 6. ä¿å­˜
image.save("output_pose.png")
print("å·²ä¿å­˜ä¸º output_pose.png")
```

---

## ğŸ“Š 4. CLIP Score åœ¨çº¿ Gateï¼ˆ>0.30 æ”¾è¡Œï¼‰
```python
# clip_score.py
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

image = Image.open("output_pose.png")
text = "a golden monkey, S* outfit"
inputs = processor(images=image, text=text, return_tensors=True)

with torch.no_grad():
    image_feat = model.get_image_features(**inputs)
    text_feat  = model.get_text_features(**inputs)
    score = (image_feat @ text_feat.T).squeeze() / (image_feat.norm() * text_feat.norm())

print("CLIP score:", score.item())   # >0.30 æ”¾è¡Œ
```

---

## ğŸ§ª 5. é›¶å·ç§¯æ®‹å·®å›¾ï¼ˆçœ¼è§ä¸ºå®ï¼‰
```python
# åœ¨ generate_pose.py é‡Œæ’ hook
residual_log = []
def hook(m, inp, out):
    down, mid = out
    down_vis = [torch.abs(d[0, 0:1]).clamp(0, 1) for d in down]
    mid_vis  = torch.abs(mid[0, 0:1]).clamp(0, 1)
    residual_log.append((down_vis, mid_vis, torch.mean(torch.abs(mid)).item()))

for blk in controlnet.down_blocks:
    blk.register_forward_hook(hook)
controlnet.mid_block.register_forward_hook(hook)

# ç”Ÿæˆå
for step, (down, mid, mean) in enumerate(residual_log):
    grid = torchvision.utils.make_grid(down + [mid], nrow=4, padding=2)
    torchvision.utils.save_image(grid, f"residual_step_{step}.png")
    print(f"step={step}  mid_block_mean={mean:.4f}")   # çœ¼è§ä¸ºå®
```


